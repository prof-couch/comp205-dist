{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "<img align=\"right\" style=\"padding-left:10px; height: 20%; width: 20%;\" src=\"figures/raindrops.jpg\">\n",
    "\n",
    "* Clustering is the process of discovering categories of data in a multi-dimensional dataset. \n",
    "* There are several kinds of clustering possible, we study just one type. \n",
    "* *Agglomerative clustering* builds categories of data points based upon their distance from each other according to some metric.  It groups the two closest points, and continues doing this until the result is one big blob. One can then divide the data into any number of categories by dividing up the agglomeration. _Sort of like, after some rain, how raindrops coalesce!_\n",
    "* Clustering is an example of *unsupervised learning*. There is no preconceived notion of the categories that should arise. Contrast with classification, where there _is_ a preconceived notion of the categories.\n",
    "\n",
    "This code is adapted from: https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, let's input some 2-d data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([[5, 3],\n",
    "                 [10, 15],\n",
    "                 [70, 55],\n",
    "                 [15, 12],\n",
    "                 [71, 80],\n",
    "                 [24, 10],\n",
    "                 [30, 30],\n",
    "                 [85, 70],\n",
    "                 [60, 78],\n",
    "                 [80, 91], ])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then let's label rows for a demo\n",
    "\n",
    "* This is a trick to label points with characters\n",
    "* `ord(character)` is the integer offset of `character` in ASCII. \n",
    "* `chr(integer)` is the character corresponding to the offset `integer`. \n",
    "* `chr(ord('a')+i)` is the `i`th character in the alphabet, starting at `i=0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_labels = [chr(c+ord('a')) for c in range(len(data))]\n",
    "row_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's put this into a DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data, index=row_labels, columns=('x', 'y'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caveat about pandas and numpy\n",
    "* It is good to remember that `numpy` predates `pandas`. \n",
    "* Some libraries take only `numpy` input. \n",
    "* Others tolerate either `pandas` or `numpy` input. \n",
    "* In this case, we have libraries that tolerate both formats. \n",
    "\n",
    "# Let's visualize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 6))  # set size of overall figure\n",
    "plt.subplots_adjust(bottom=0.1)  \n",
    "\n",
    "# make a scatterplot of all data,\n",
    "plt.scatter(df.loc[:,'x'],df.loc[:,'y'], label='True Position')\n",
    "\n",
    "# label points with their letters for clarity. \n",
    "# use parallel iteration over three parallel lists \n",
    "for label, x, y in zip(df.index, df.loc[:, 'x'], df.loc[:, 'y']):  \n",
    "    plt.annotate(\n",
    "        label, # label of text\n",
    "        xy=(x, y),  # where point is in coordinate space \n",
    "        xytext=(-3, 3),  # offset of label in pixels\n",
    "        textcoords='offset points', # how to interpret coordinates \n",
    "        ha='right',  # text horizontal alignment\n",
    "        va='bottom')  # text vertical alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some basic observations\n",
    "* It's pretty obvious that this data is segregated into two categories. \n",
    "* It's not so obvious as to how to do this by machine. \n",
    "* The following is an illustration of one way to do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage  \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# make a hierarchical structure from the data. \n",
    "linked = linkage(data, 'single')\n",
    "\n",
    "# plot the hierarchy as a 'dendrogram' \n",
    "plt.figure(figsize=(9, 6))  \n",
    "_ = dendrogram(linked,  \n",
    "               orientation='top',\n",
    "               labels=row_labels,\n",
    "               distance_sort='descending',\n",
    "               show_leaf_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agglomerative clustering\n",
    "The above diagram is made as follows: \n",
    "1. Start with nothing linked. Every point is in its own cluster. \n",
    "2. Link the two closest clusters to one another. The distance between clusters is the minimum distance between points in the cluster. \n",
    "3. Continue until you have one large cluster.  \n",
    "\n",
    "We usually skip showing the previous diagram, and proceed as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Compute clusters\n",
    "cluster2 = AgglomerativeClustering(n_clusters=2, affinity='euclidean')\n",
    "cluster2.fit_predict(df)\n",
    "\n",
    "# color each cluster with a different color of the rainbow\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(df.loc[:, 'x'],  # x coordinate\n",
    "            df.loc[:, 'y'],  # y coordinate\n",
    "            c=cluster2.labels_,  # how to determine colors of points.\n",
    "            cmap='rainbow')  # what color code to use\n",
    "\n",
    "# Label data points with their letters\n",
    "for label, x, y in zip(df.index, df.loc[:, 'x'], df.loc[:, 'y']):\n",
    "    plt.annotate(\n",
    "        label,  # what label to place\n",
    "        xy=(x, y),  # where the point is (actual coordinates)\n",
    "        xytext=(-3, 3),  # where the characters should start (pixels)\n",
    "        textcoords='offset points',  # use the xytext offsets to place text\n",
    "        ha='right',  # horizontal alignment of text\n",
    "        va='bottom')  # vertical alignment of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How this was done\n",
    "* in `plt.scatter`, change the color for each cluster: `c=cluster2.labels_`\n",
    "* Map each cluster to a color of the rainbow: `cmap='rainbow'`\n",
    "        \n",
    "Here's the same chart for _three clusters_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster3 = AgglomerativeClustering(n_clusters=3, affinity='euclidean')  \n",
    "cluster3.fit_predict(df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(df.loc[:,'x'],df.loc[:,'y'], c=cluster3.labels_, cmap='rainbow')  \n",
    "for label, x, y in zip(df.index, df.loc[:, 'x'], df.loc[:, 'y']):  \n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y), \n",
    "        xytext=(-3, 3),\n",
    "        textcoords='offset points', \n",
    "        ha='right', \n",
    "        va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data is often not so well behaved\n",
    "* There can be nore than two axes. \n",
    "* Clusters may not be separable. \n",
    "* Human-specified and data-driven categories need not match. \n",
    "* The reason is the scourge of the data scientist: *hidden variables*.\n",
    "\n",
    "# Hidden variables\n",
    "* Often, the human categorization of something differs from the machine categorization due to specialized knowledge. \n",
    "* No dataset is complete. \n",
    "* There are aspects that are not recorded. \n",
    "* These can determine categories that the data doesn't depict. \n",
    "\n",
    "# Let's try this out\n",
    "* Here is a practical case study of hidden variables. \n",
    "* The Iris dataset doesn't in any sense contain everything about irises. \n",
    "* We can try clustering it, and compare that with the human categorization. \n",
    "* We'll get somewhat different results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this cell; just run it. \n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('Clustering.ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Call Agglomerative Clustering on the iris data, as above, with three categories. Put the result into the variable `cluster`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer: \n",
    "cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean')  \n",
    "cluster.fit_predict(iris.data)\n",
    "cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this with the expert decisions in `iris.target` \n",
    "Category numbers are arbitrary. You'll need to map between numbering systems to compare these. \n",
    "\n",
    "2. Compare the above output with `iris.target` to determine the closest match between category numbers. \n",
    "What category numbers in your map correspond to the ones in `iris.target`? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Your answer:___\n",
    "* `iris.target` category 0 corresponds to `cluster.labels_` category ______1__________.\n",
    "* `iris.target` category 1 corresponds to `cluster.labels_` category ______0___________.\n",
    "* `iris.target` category 2 corresponds to `cluster.labels_` category ______2___________."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This situation is very common. The two categorizations are slightly different. We want to know how close they can be matched to one another. This is a measure of how much information is hidden that we are not recording or using in clustering. \n",
    "\n",
    "3. Create an `array` `comparable` that maps categories you have listed above to their closest equivalents in `iris.target`. You can do this with a `for` loop or become more clever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer: \n",
    "import numpy as np\n",
    "out = list()\n",
    "for i in cluster.labels_: \n",
    "    if i == 1: \n",
    "        out.append(0)\n",
    "    elif i==0: \n",
    "        out.append(1)\n",
    "    else: \n",
    "        out.append(2)\n",
    "comparable = np.array(out)\n",
    "comparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q03')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute the number of identical characterizations. Hint: use array comparison, and note that the sum of a Boolean vector is the number of True values. Put the result in `same`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer: \n",
    "same = (comparable == iris.target).sum()\n",
    "same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to understand that this isn't an error of clustering. Clustering acts on the data it gets. The problem is that something is missing. What is missing distinguishes two categories of `iris.target` more clearly. \n",
    "\n",
    "4. Which categories in `iris.target` are not clearly distinguished? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Your answer:___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we know something that's missing: the human categorization! Will adding that to the data fix the clustering? Let's try it as an educational exercise. \n",
    "\n",
    "5. Run the same experiment on the array created by combining `iris.data` and `iris.target` so that we have five columns of input data. Print the number of differences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer: \n",
    "newdata = np.insert(iris.data,4,iris.target, axis=1)\n",
    "cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean')  \n",
    "cluster.fit_predict(newdata)\n",
    "out = list()\n",
    "for i in cluster.labels_: \n",
    "    if i == 1: \n",
    "        out.append(0)\n",
    "    elif i==2: \n",
    "        out.append(1)\n",
    "    else: \n",
    "        out.append(2)\n",
    "cats = np.array(out)\n",
    "same = (cats == iris.target).sum()\n",
    "same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moral of this story is that no clustering mechanism is perfect, and no clustering mechanism truly substitutes for human categorization. Clustering mechanisms can get close, but are never completely accurate. The last experiment begs the question. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
